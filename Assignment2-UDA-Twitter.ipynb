{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics for Unstructured Data: Group Assignment #2\n",
    "\n",
    "| **S.no** \t|    **Team Member**    \t| **EID**|\n",
    "|:--------:\t|:---------------------:|:----:|\n",
    "|     1    \t|Amrit Pradhan|ap58785|\n",
    "|     2    \t|Deeksha Pandit|dp35222|\n",
    "|     3    \t|Meghna PM|mp49453|\n",
    "|     4    \t|Sanjo Shaju|ss99958|\n",
    "|     5    \t|Shivangi Dubey|scd2422|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter scrapper library used: \n",
    "https://github.com/JustAnotherArchivist/snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the library\n",
    "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "hashtag = [\"2022Midterms\", \"PA2022\", \"PAGov2022\",\"PaSenate\",\"PASenate\",\"PASen\",\"MidtermElections\",\"papolitics\",\"RegisterToVote\"] #hashtag to scrape\n",
    "date_interval = [\"2022-07-01\", \"2022-08-01\", \"2022-09-01\", \"2022-10-01\"] #to ensure there are tweets from different month\n",
    "\n",
    "arg = \"twitter-hashtag\"\n",
    "\n",
    "data = None\n",
    "for i in range(len(date_interval)-1):\n",
    "    for hash in hashtag:\n",
    "        os.system(\n",
    "            f'snscrape --jsonl --progress --max-results 10000 --since {date_interval[i]} {arg} \"{hash} until:{date_interval[i+1]}\" > text-query-tweets.json')\n",
    "        tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "        try:\n",
    "            df = tweets_df[[\"id\", \"url\", \"date\", \"rawContent\", \"renderedContent\",\n",
    "                            \"hashtags\", \"cashtags\", \"media\", \"lang\",\"place\",\"coordinates\"]]\n",
    "            if data is None:\n",
    "                data = df\n",
    "            else:\n",
    "                data = data.append(df)\n",
    "            print(f\"{hash} ({date_interval[i]}->{date_interval[i+1]}): {len(df)} \")\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "data.to_csv(\"PA_election.csv\",index=False) # storing the data so that we dont have to run the scrapper again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scrapping an initial set of tweets we did an frequency analysis to identify other common hastags to scrape tweets from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3 = [item for sublist in data[\"hashtags\"].to_numpy() for item in sublist]\n",
    "\n",
    "cols = [\"word\", \"count\"]\n",
    "lst = []\n",
    "def countFreq(arr, n):\n",
    "\n",
    "    mp = dict()\n",
    "    for i in range(n):\n",
    "        if arr[i] in mp.keys():\n",
    "            mp[arr[i]] += 1\n",
    "        else:\n",
    "            mp[arr[i]] = 1\n",
    "\n",
    "    for x in mp:\n",
    "        lst.append([x, mp[x]])\n",
    "\n",
    "countFreq(list3, len(list3))\n",
    "df1 = pd.DataFrame(lst, columns=cols)\n",
    "df1.sort_values(by=['count'],inplace=True, ascending=False)\n",
    "df1.to_csv(\"commonlyusedhastags.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>rawContent</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>media</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.55389E+18</td>\n",
       "      <td>https://twitter.com/NetBum1/status/15538906281...</td>\n",
       "      <td>2022-07-31 23:49:30+00:00</td>\n",
       "      <td>@adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...</td>\n",
       "      <td>@adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...</td>\n",
       "      <td>['2022MIDTERMS', '2A', 'NOMOREGUNLAWS', 'REPEA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.55389E+18</td>\n",
       "      <td>https://twitter.com/NetBum1/status/15538898974...</td>\n",
       "      <td>2022-07-31 23:46:36+00:00</td>\n",
       "      <td>@adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...</td>\n",
       "      <td>@adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...</td>\n",
       "      <td>['2022MIDTERMS', '2A', 'BRINGBACKTRUMP', 'VOTE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.55389E+18</td>\n",
       "      <td>https://twitter.com/todaysnews2go/status/15538...</td>\n",
       "      <td>2022-07-31 23:39:16+00:00</td>\n",
       "      <td>Private Prison Sues State for Not Having Enoug...</td>\n",
       "      <td>Private Prison Sues State for Not Having Enoug...</td>\n",
       "      <td>['election', 'economy', '2022midterms']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.55388E+18</td>\n",
       "      <td>https://twitter.com/drksingleton80/status/1553...</td>\n",
       "      <td>2022-07-31 23:07:41+00:00</td>\n",
       "      <td>Can we just admit that the American food lifes...</td>\n",
       "      <td>Can we just admit that the American food lifes...</td>\n",
       "      <td>['Insulin4all', '2022midterms']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.55387E+18</td>\n",
       "      <td>https://twitter.com/todaysnews2go/status/15538...</td>\n",
       "      <td>2022-07-31 22:39:19+00:00</td>\n",
       "      <td>Kudos To Karine https://t.co/2A2J86oSyq #infla...</td>\n",
       "      <td>Kudos To Karine bit.ly/3cItPUL #inflation #ele...</td>\n",
       "      <td>['inflation', 'election', '2022midterms']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21869</th>\n",
       "      <td>1.56515E+18</td>\n",
       "      <td>https://twitter.com/awokonewspaper/status/1565...</td>\n",
       "      <td>2022-09-01 01:21:34+00:00</td>\n",
       "      <td>News is what we sell, not propaganda ... Get o...</td>\n",
       "      <td>News is what we sell, not propaganda ... Get o...</td>\n",
       "      <td>['SierraLeone', 'SaloneTwitter', 'Freetown', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', '...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21870</th>\n",
       "      <td>1.56514E+18</td>\n",
       "      <td>https://twitter.com/AlisonDeLuca/status/156514...</td>\n",
       "      <td>2022-09-01 00:55:32+00:00</td>\n",
       "      <td>https://t.co/SHwHufAGmK #RegisterToVote</td>\n",
       "      <td>IWillVote.com #RegisterToVote</td>\n",
       "      <td>['RegisterToVote']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21871</th>\n",
       "      <td>1.56514E+18</td>\n",
       "      <td>https://twitter.com/KorbelRenoKia/status/15651...</td>\n",
       "      <td>2022-09-01 00:46:30+00:00</td>\n",
       "      <td>@allinwithchris @chrislhayes Wow. It only took...</td>\n",
       "      <td>@allinwithchris @chrislhayes Wow. It only took...</td>\n",
       "      <td>['Alaska', 'RegisterToVote', 'VoteBlue2022']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21872</th>\n",
       "      <td>1.56514E+18</td>\n",
       "      <td>https://twitter.com/dittycommittee/status/1565...</td>\n",
       "      <td>2022-09-01 00:38:40+00:00</td>\n",
       "      <td>#RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...</td>\n",
       "      <td>#RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...</td>\n",
       "      <td>['RegisterToVote', 'VoteBlueToSaveDemocracy', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qht</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21873</th>\n",
       "      <td>1.56513E+18</td>\n",
       "      <td>https://twitter.com/CIMAGES/status/15651336160...</td>\n",
       "      <td>2022-09-01 00:25:07+00:00</td>\n",
       "      <td>Wake up #America, there are only 69 days left ...</td>\n",
       "      <td>Wake up #America, there are only 69 days left ...</td>\n",
       "      <td>['America', 'VoteBlueToSaveDemocracy', 'Republ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', '...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21874 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                                url  \\\n",
       "0      1.55389E+18  https://twitter.com/NetBum1/status/15538906281...   \n",
       "1      1.55389E+18  https://twitter.com/NetBum1/status/15538898974...   \n",
       "2      1.55389E+18  https://twitter.com/todaysnews2go/status/15538...   \n",
       "3      1.55388E+18  https://twitter.com/drksingleton80/status/1553...   \n",
       "4      1.55387E+18  https://twitter.com/todaysnews2go/status/15538...   \n",
       "...            ...                                                ...   \n",
       "21869  1.56515E+18  https://twitter.com/awokonewspaper/status/1565...   \n",
       "21870  1.56514E+18  https://twitter.com/AlisonDeLuca/status/156514...   \n",
       "21871  1.56514E+18  https://twitter.com/KorbelRenoKia/status/15651...   \n",
       "21872  1.56514E+18  https://twitter.com/dittycommittee/status/1565...   \n",
       "21873  1.56513E+18  https://twitter.com/CIMAGES/status/15651336160...   \n",
       "\n",
       "                            date  \\\n",
       "0      2022-07-31 23:49:30+00:00   \n",
       "1      2022-07-31 23:46:36+00:00   \n",
       "2      2022-07-31 23:39:16+00:00   \n",
       "3      2022-07-31 23:07:41+00:00   \n",
       "4      2022-07-31 22:39:19+00:00   \n",
       "...                          ...   \n",
       "21869  2022-09-01 01:21:34+00:00   \n",
       "21870  2022-09-01 00:55:32+00:00   \n",
       "21871  2022-09-01 00:46:30+00:00   \n",
       "21872  2022-09-01 00:38:40+00:00   \n",
       "21873  2022-09-01 00:25:07+00:00   \n",
       "\n",
       "                                              rawContent  \\\n",
       "0      @adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...   \n",
       "1      @adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...   \n",
       "2      Private Prison Sues State for Not Having Enoug...   \n",
       "3      Can we just admit that the American food lifes...   \n",
       "4      Kudos To Karine https://t.co/2A2J86oSyq #infla...   \n",
       "...                                                  ...   \n",
       "21869  News is what we sell, not propaganda ... Get o...   \n",
       "21870            https://t.co/SHwHufAGmK #RegisterToVote   \n",
       "21871  @allinwithchris @chrislhayes Wow. It only took...   \n",
       "21872  #RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...   \n",
       "21873  Wake up #America, there are only 69 days left ...   \n",
       "\n",
       "                                         renderedContent  \\\n",
       "0      @adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...   \n",
       "1      @adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...   \n",
       "2      Private Prison Sues State for Not Having Enoug...   \n",
       "3      Can we just admit that the American food lifes...   \n",
       "4      Kudos To Karine bit.ly/3cItPUL #inflation #ele...   \n",
       "...                                                  ...   \n",
       "21869  News is what we sell, not propaganda ... Get o...   \n",
       "21870                      IWillVote.com #RegisterToVote   \n",
       "21871  @allinwithchris @chrislhayes Wow. It only took...   \n",
       "21872  #RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...   \n",
       "21873  Wake up #America, there are only 69 days left ...   \n",
       "\n",
       "                                                hashtags cashtags  \\\n",
       "0      ['2022MIDTERMS', '2A', 'NOMOREGUNLAWS', 'REPEA...      NaN   \n",
       "1      ['2022MIDTERMS', '2A', 'BRINGBACKTRUMP', 'VOTE...      NaN   \n",
       "2                ['election', 'economy', '2022midterms']      NaN   \n",
       "3                        ['Insulin4all', '2022midterms']      NaN   \n",
       "4              ['inflation', 'election', '2022midterms']      NaN   \n",
       "...                                                  ...      ...   \n",
       "21869  ['SierraLeone', 'SaloneTwitter', 'Freetown', '...      NaN   \n",
       "21870                                 ['RegisterToVote']      NaN   \n",
       "21871       ['Alaska', 'RegisterToVote', 'VoteBlue2022']      NaN   \n",
       "21872  ['RegisterToVote', 'VoteBlueToSaveDemocracy', ...      NaN   \n",
       "21873  ['America', 'VoteBlueToSaveDemocracy', 'Republ...      NaN   \n",
       "\n",
       "                                                   media lang place  \\\n",
       "0                                                    NaN  qme   NaN   \n",
       "1                                                    NaN  qme   NaN   \n",
       "2                                                    NaN   en   NaN   \n",
       "3                                                    NaN   en   NaN   \n",
       "4                                                    NaN   lt   NaN   \n",
       "...                                                  ...  ...   ...   \n",
       "21869  [{'_type': 'snscrape.modules.twitter.Photo', '...   en   NaN   \n",
       "21870                                                NaN  qme   NaN   \n",
       "21871                                                NaN   en   NaN   \n",
       "21872                                                NaN  qht   NaN   \n",
       "21873  [{'_type': 'snscrape.modules.twitter.Photo', '...   en   NaN   \n",
       "\n",
       "      coordinates  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "21869         NaN  \n",
       "21870         NaN  \n",
       "21871         NaN  \n",
       "21872         NaN  \n",
       "21873         NaN  \n",
       "\n",
       "[21874 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the tweets back\n",
    "df_tweets = pd.read_csv(\"PA_election.csv\")\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "from multiprocessing import  Pool\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# https://stackoverflow.com/a/34682849\n",
    "def untokenize(words):\n",
    "    \"\"\"Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/47091490\n",
    "def decontracted(phrase):\n",
    "    \"\"\"Convert contractions like \"can't\" into \"can not\"\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    #phrase = re.sub(r\"n't\", \" not\", phrase) # resulted in \"ca not\" when sentence started with \"can't\"\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "# https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "slang_abbrev_dict = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': 'For What It\\'s Worth',\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you',\n",
    "    'ILU': 'I Love You',\n",
    "    'IMHO': 'In My Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My Ass Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'OMG': 'Oh My God',\n",
    "    'PITA': 'Pain In The Ass',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA?': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The Fuck',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait',\n",
    "    '7K': 'Sick:-D Laugher'\n",
    "}\n",
    "\n",
    "\n",
    "def unslang(text):\n",
    "    \"\"\"Converts text like \"OMG\" into \"Oh my God\"\n",
    "    \"\"\"\n",
    "    if text.upper() in slang_abbrev_dict.keys():\n",
    "        return slang_abbrev_dict[text.upper()]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "# https://gist.github.com/sebleier/554280\n",
    "stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\",\n",
    "    \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\",\n",
    "    \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\n",
    "    \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\",\n",
    "    \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\",\n",
    "    \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\",\n",
    "    \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "    \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\",\n",
    "    \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\",\n",
    "    \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\",\n",
    "    \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\",\n",
    "    \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
    "    \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\",\n",
    "    \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\",\n",
    "    \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\",\n",
    "    \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\",\n",
    "    \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\",\n",
    "    \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\",\n",
    "    \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\",\n",
    "    \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\",\n",
    "    \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\",\n",
    "    \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n",
    "    \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\",\n",
    "    \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\n",
    "    \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\",\n",
    "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\",\n",
    "    \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\",\n",
    "    \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\",\n",
    "    \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\",\n",
    "    \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\",\n",
    "    \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\",\n",
    "    \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\",\n",
    "    \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\",\n",
    "    \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\",\n",
    "    \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\",\n",
    "    \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n",
    "    \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\",\n",
    "    \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\",\n",
    "    \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\",\n",
    "    \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\",\n",
    "    \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\",\n",
    "    \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\",\n",
    "    \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\",\n",
    "    \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\",\n",
    "    \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\",\n",
    "    \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\",\n",
    "    \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\",\n",
    "    \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\",\n",
    "    \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\",\n",
    "    \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\",\n",
    "    \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\",\n",
    "    \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\",\n",
    "    \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n",
    "    \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\",\n",
    "    \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\",\n",
    "    \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\",\n",
    "    \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\",\n",
    "    \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\",\n",
    "    \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\",\n",
    "    \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\",\n",
    "    \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\",\n",
    "    \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\",\n",
    "    \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\",\n",
    "    \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\",\n",
    "    \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\",\n",
    "    \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\",\n",
    "    \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\",\n",
    "    \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\",\n",
    "    \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\",\n",
    "    \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n",
    "    \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\",\n",
    "    \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\",\n",
    "    \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\",\n",
    "    \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\",\n",
    "    \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\",\n",
    "    \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\",\n",
    "    \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\",\n",
    "    \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\",\n",
    "    \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\",\n",
    "    \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\",\n",
    "    \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\",\n",
    "    \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\",\n",
    "    \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\",\n",
    "    \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\",\n",
    "    \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\",\n",
    "    \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\",\n",
    "    \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\",\n",
    "    \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\",\n",
    "    \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\",\n",
    "    \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\",\n",
    "    \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\",\n",
    "    \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\",\n",
    "    \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\",\n",
    "    \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\",\n",
    "    \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\",\n",
    "    \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\",\n",
    "    \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\",\n",
    "    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\",\n",
    "    \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\n",
    "    \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\",\n",
    "    \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\",\n",
    "    \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\n",
    "    \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\",\n",
    "    \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\",\n",
    "    \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\",\n",
    "    \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\",\n",
    "    \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\",\n",
    "    \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\",\n",
    "    \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\",\n",
    "    \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\",\n",
    "    \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n",
    "    \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\",\n",
    "    \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
    "    \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "    \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
    "    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\",\n",
    "    \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\",\n",
    "    \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\n",
    "    \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
    "    \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "    \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\",\n",
    "    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\n",
    "    \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\",\n",
    "    \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\",\n",
    "    \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
    "    \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\",\n",
    "    \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n",
    "    \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\",\n",
    "    \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\",\n",
    "    \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\",\n",
    "    \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\",\n",
    "    \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\",\n",
    "    \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\",\n",
    "    \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\",\n",
    "    \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\",\n",
    "    \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\",\n",
    "    \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\",\n",
    "    \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\",\n",
    "    \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\",\n",
    "    \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\",\n",
    "    \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\",\n",
    "    \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\",\n",
    "    \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\",\n",
    "    \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\",\n",
    "    \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\",\n",
    "    \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\",\n",
    "    \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\",\n",
    "    \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\",\n",
    "    \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\",\n",
    "    \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\",\n",
    "    \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\",\n",
    "    \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\",\n",
    "    \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\",\n",
    "    \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\",\n",
    "    \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\",\n",
    "    \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\",\n",
    "    \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\",\n",
    "    \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\",\n",
    "    \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"\n",
    "]\n",
    "\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "# from: https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
    "# maybe a bug, it removes question marks?\n",
    "# spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = clean(r\"http\\S+\", text)\n",
    "    text = clean(r\"www\\S+\", text)\n",
    "    text = clean(r\"pic.twitter.com\\S+\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean(reg_exp, text):\n",
    "    text = re.sub(reg_exp, \" \", text)\n",
    "\n",
    "    # replace multiple spaces with one.\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_all(t, correct_spelling=False, remove_stopwords=True, lemmatize=True):\n",
    "    try:\n",
    "        # To lowercase\n",
    "        t = t.lower()\n",
    "        \n",
    "        # first do bulk cleanup on tokens that don't depend on word tokenization\n",
    "\n",
    "        # remove xml tags\n",
    "        t = clean(r\"<[^>]+>\", t)\n",
    "        t = clean(r\"&lt;\", t)\n",
    "        t = clean(r\"&gt;\", t)\n",
    "\n",
    "        # remove URLs\n",
    "        t = remove_urls(t)\n",
    "\n",
    "        # https://stackoverflow.com/a/35041925\n",
    "        # replace multiple punctuation with single. Ex: !?!?!? would become ?\n",
    "        t = clean(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', t)\n",
    "\n",
    "        t = remove_emoji(t)\n",
    "\n",
    "        # expand common contractions like \"I'm\" \"he'll\"\n",
    "        t = decontracted(t)\n",
    "\n",
    "        # now remove/expand bad patterns per word\n",
    "        words = word_tokenize(t)\n",
    "\n",
    "        # remove stopwords\n",
    "        if remove_stopwords is True:\n",
    "            words = [w for w in words if not w in stopwords]\n",
    "\n",
    "        clean_words = []\n",
    "\n",
    "        for w in words:\n",
    "            # normalize punctuation\n",
    "            w = re.sub(r'&', 'and', w)\n",
    "\n",
    "            # expand slang like OMG = Oh my God\n",
    "            w = unslang(w)\n",
    "\n",
    "            if lemmatize is True:\n",
    "                w = lemmatizer.lemmatize(w)\n",
    "            \n",
    "            clean_words.append(w)\n",
    "\n",
    "        # join the words back into a full string\n",
    "        t = untokenize(clean_words)\n",
    "\n",
    "        if correct_spelling is True:\n",
    "            # this resulted in lots of lost punctuation - omitting for now. Also greatly speeds things up\n",
    "            t = correct_spellings(t)\n",
    "\n",
    "        # finally, remove any non ascii and special characters that made it through\n",
    "        t = clean(r\"[^A-Za-z0-9\\.\\'!\\?,\\$]\", t)\n",
    "    except:\n",
    "        return t\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def clean_dataframe(df, correct_spelling=False, remove_stopwords=True):\n",
    "    df['clean'] = df.apply(lambda x: clean_all(x['renderedContent'], correct_spelling=correct_spelling, remove_stopwords=remove_stopwords), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1\n",
    "def parallelize_dataframe(\n",
    "        df, func, n_cores=2):  # I think Kaggle notebooks only have 2 cores?\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df_temp_tweets = df_tweets.copy()\n",
    "# Clean the tweets\n",
    "df_temp_tweets['clean'] = df_temp_tweets.apply(lambda x: clean_all(x['renderedContent']), axis=1)\n",
    "# Clean the place data\n",
    "df_temp_tweets['place'] = df_temp_tweets['place'].fillna('').apply(lambda x: ast.literal_eval(x)['fullName'] if x != \"\" else \"\")\n",
    "\n",
    "df_temp_tweets = df_temp_tweets[['renderedContent', 'clean','place']]\n",
    "df_temp_tweets.dropna(inplace=True)\n",
    "# df_temp_tweets.to_csv(\"csv_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the city/town names with \"smalltownpa\" and \"bigtownpa\" keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aliquippa': 'smalltownpa',\n",
       " 'Allentown': 'bigtownpa',\n",
       " 'Altoona': 'smalltownpa',\n",
       " 'Arnold': 'smalltownpa',\n",
       " 'Beaver Falls': 'smalltownpa',\n",
       " 'Bethlehem': 'smalltownpa',\n",
       " 'Bloomsburg': 'smalltownpa',\n",
       " 'Bradford': 'smalltownpa',\n",
       " 'Butler': 'smalltownpa',\n",
       " 'Carbondale': 'smalltownpa',\n",
       " 'Chester': 'smalltownpa',\n",
       " 'Clairton': 'smalltownpa',\n",
       " 'Coatesville': 'smalltownpa',\n",
       " 'Connellsville': 'smalltownpa',\n",
       " 'Corry': 'smalltownpa',\n",
       " 'Du Bois': 'smalltownpa',\n",
       " 'Duquesne': 'smalltownpa',\n",
       " 'Easton': 'smalltownpa',\n",
       " 'Erie': 'bigtownpa',\n",
       " 'Farrell': 'smalltownpa',\n",
       " 'Franklin': 'smalltownpa',\n",
       " 'Greensburg': 'smalltownpa',\n",
       " 'Harrisburg': 'smalltownpa',\n",
       " 'Hazleton': 'smalltownpa',\n",
       " 'Hermitage': 'smalltownpa',\n",
       " 'Jeannette': 'smalltownpa',\n",
       " 'Johnstown': 'smalltownpa',\n",
       " 'Lancaster': 'smalltownpa',\n",
       " 'Latrobe': 'smalltownpa',\n",
       " 'Lebanon': 'smalltownpa',\n",
       " 'Lock Haven': 'smalltownpa',\n",
       " 'Lower Burrell': 'smalltownpa',\n",
       " 'McCandless': 'smalltownpa',\n",
       " 'McKeesport': 'smalltownpa',\n",
       " 'Meadville': 'smalltownpa',\n",
       " 'Monessen': 'smalltownpa',\n",
       " 'Monongahela': 'smalltownpa',\n",
       " 'Nanticoke': 'smalltownpa',\n",
       " 'New Castle': 'smalltownpa',\n",
       " 'New Kensington': 'smalltownpa',\n",
       " 'Oil City': 'smalltownpa',\n",
       " 'Parker': 'smalltownpa',\n",
       " 'Philadelphia': 'bigtownpa',\n",
       " 'Pittsburgh': 'bigtownpa',\n",
       " 'Pittston': 'smalltownpa',\n",
       " 'Pottsville': 'smalltownpa',\n",
       " 'Reading': 'bigtownpa',\n",
       " 'St. Marys': 'smalltownpa',\n",
       " 'Scranton': 'smalltownpa',\n",
       " 'Shamokin': 'smalltownpa',\n",
       " 'Sharon': 'smalltownpa',\n",
       " 'Sunbury': 'smalltownpa',\n",
       " 'Titusville': 'smalltownpa',\n",
       " 'Uniontown': 'smalltownpa',\n",
       " 'Warren': 'smalltownpa',\n",
       " 'Washington': 'smalltownpa',\n",
       " 'Wilkes-Barre': 'smalltownpa',\n",
       " 'Williamsport': 'smalltownpa',\n",
       " 'York': 'smalltownpa'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_replace = pd.read_csv('city_town_classifier.csv')\n",
    "town_map = dict(zip(df_replace.Town, df_replace.Replacement))\n",
    "town_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_map = {'Aliquippa': 'smalltownpa','Allentown': 'bigtownpa','Altoona': 'smalltownpa','Arnold': 'smalltownpa','Beaver Falls': 'smalltownpa','Bethlehem': 'smalltownpa','Bloomsburg': 'smalltownpa','Bradford': 'smalltownpa','Butler': 'smalltownpa','Carbondale': 'smalltownpa','Chester': 'smalltownpa','Clairton': 'smalltownpa','Coatesville': 'smalltownpa','Connellsville': 'smalltownpa','Corry': 'smalltownpa','Du Bois': 'smalltownpa','Duquesne': 'smalltownpa','Easton': 'smalltownpa','Erie': 'bigtownpa','Farrell': 'smalltownpa','Franklin': 'smalltownpa','Greensburg': 'smalltownpa','Harrisburg': 'smalltownpa','Hazleton': 'smalltownpa','Hermitage': 'smalltownpa','Jeannette': 'smalltownpa',\n",
    "                'Johnstown': 'smalltownpa','Lancaster': 'smalltownpa','Latrobe': 'smalltownpa','Lebanon': 'smalltownpa','Lock Haven': 'smalltownpa','Lower Burrell': 'smalltownpa','McCandless': 'smalltownpa','McKeesport': 'smalltownpa','Meadville': 'smalltownpa','Monessen': 'smalltownpa','Monongahela': 'smalltownpa','Nanticoke': 'smalltownpa','New Castle': 'smalltownpa','New Kensington': 'smalltownpa','Oil City': 'smalltownpa','Parker': 'smalltownpa','Philadelphia': 'bigtownpa','Pittsburgh': 'bigtownpa','Pittston': 'smalltownpa','Pottsville': 'smalltownpa','Reading': 'bigtownpa',\n",
    "                'St. Marys': 'smalltownpa','Scranton': 'smalltownpa','Shamokin': 'smalltownpa','Sharon': 'smalltownpa','Sunbury': 'smalltownpa','Titusville': 'smalltownpa','Uniontown': 'smalltownpa','Warren': 'smalltownpa','Washington': 'smalltownpa','Wilkes-Barre': 'smalltownpa','Williamsport': 'smalltownpa','York': 'smalltownpa'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_town_classifier.csv has the info on how we classified towns/city to small or big ones in PA\n",
    "# df_replace = pd.read_csv('city_town_classifier.csv')\n",
    "# town_map = dict(zip(df_replace.Town, df_replace.Replacement))\n",
    "\n",
    "def replace(match):\n",
    "    return town_map[match.group(0)]\n",
    "\n",
    "def replace_town_names(text):\n",
    "    text = re.sub('|'.join(r'\\b%s\\b' % re.escape(s) for s in town_map), replace, text)\n",
    "    if text[:-4] in ['smalltownpa','bigtownpa']:\n",
    "        return text[:-4]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Adding the location keywords to the tweets\n",
    "df_temp_tweets['town'] = df_temp_tweets['place'].map(replace_town_names)\n",
    "df_temp_tweets['clean'] = df_temp_tweets['clean'].map(str) + \" \" + df_temp_tweets['town'].map(str)\n",
    "\n",
    "df_temp_tweets = df_temp_tweets[['renderedContent', 'clean']]\n",
    "# df_temp_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1</td>\n",
       "      <td>registertovote</td>\n",
       "      <td>9460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2</td>\n",
       "      <td>vote</td>\n",
       "      <td>5568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>3</td>\n",
       "      <td>pasen</td>\n",
       "      <td>5421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2022midterms</td>\n",
       "      <td>3572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>election</td>\n",
       "      <td>2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>39852</td>\n",
       "      <td>deposit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>39853</td>\n",
       "      <td>exploring</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>39854</td>\n",
       "      <td>marginoffraud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21475</th>\n",
       "      <td>39855</td>\n",
       "      <td>turnbigly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39855</th>\n",
       "      <td>39856</td>\n",
       "      <td>freetown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39856 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rank            word  count\n",
       "288        1  registertovote   9460\n",
       "289        2            vote   5568\n",
       "395        3           pasen   5421\n",
       "3          4    2022midterms   3572\n",
       "23         5        election   2353\n",
       "...      ...             ...    ...\n",
       "4395   39852         deposit      1\n",
       "4396   39853       exploring      1\n",
       "21476  39854   marginoffraud      1\n",
       "21475  39855       turnbigly      1\n",
       "39855  39856        freetown      1\n",
       "\n",
       "[39856 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count frequency of words from all the scrapped comments\n",
    "comments_combined = df_temp_tweets['clean'].str.cat(sep=' ')\n",
    "comments_combined=re.sub(r'[^\\w\\s]', '', comments_combined)\n",
    "words = nltk.tokenize.word_tokenize(comments_combined)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "df_freq = pd.DataFrame(word_dist.items(), columns=['word', 'count']).sort_values(by = 'count', ascending = False)\n",
    "df_freq.insert(0, 'Rank', range(1, 1 + len(df_freq)))\n",
    "\n",
    "# storing the word frequencies to excel as backup.\n",
    "df_freq.to_csv('Word_freq.csv',index=False)\n",
    "\n",
    "df_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing various keywords related to Fetterman and Oz to just \"fetterman\" and \"oz\" keywords which we will be using to do the lift analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...</td>\n",
       "      <td>adamcarolla timcast pnjaban 2022midterms 2a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...</td>\n",
       "      <td>adamcarolla timcast pnjaban anncoulter senate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private Prison Sues State for Not Having Enoug...</td>\n",
       "      <td>private prison sue state prisoner wondergressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can we just admit that the American food lifes...</td>\n",
       "      <td>admit american food lifestyle promotes diabete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kudos To Karine bit.ly/3cItPUL #inflation #ele...</td>\n",
       "      <td>kudos karine bit.ly 3citpul inflation election...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21869</th>\n",
       "      <td>News is what we sell, not propaganda ... Get o...</td>\n",
       "      <td>news sell, propaganda. e paper straight phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21870</th>\n",
       "      <td>IWillVote.com #RegisterToVote</td>\n",
       "      <td>iwillvote.com registertovote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21871</th>\n",
       "      <td>@allinwithchris @chrislhayes Wow. It only took...</td>\n",
       "      <td>allinwithchris chrislhayes wow. 63 year. cong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21872</th>\n",
       "      <td>#RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...</td>\n",
       "      <td>registertovote votebluetosavedemocracy votebl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21873</th>\n",
       "      <td>Wake up #America, there are only 69 days left ...</td>\n",
       "      <td>wake america, 69 day left midterm. votebluetos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21873 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         renderedContent  \\\n",
       "0      @adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...   \n",
       "1      @adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...   \n",
       "2      Private Prison Sues State for Not Having Enoug...   \n",
       "3      Can we just admit that the American food lifes...   \n",
       "4      Kudos To Karine bit.ly/3cItPUL #inflation #ele...   \n",
       "...                                                  ...   \n",
       "21869  News is what we sell, not propaganda ... Get o...   \n",
       "21870                      IWillVote.com #RegisterToVote   \n",
       "21871  @allinwithchris @chrislhayes Wow. It only took...   \n",
       "21872  #RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...   \n",
       "21873  Wake up #America, there are only 69 days left ...   \n",
       "\n",
       "                                                   clean  \n",
       "0       adamcarolla timcast pnjaban 2022midterms 2a s...  \n",
       "1       adamcarolla timcast pnjaban anncoulter senate...  \n",
       "2      private prison sue state prisoner wondergressi...  \n",
       "3      admit american food lifestyle promotes diabete...  \n",
       "4      kudos karine bit.ly 3citpul inflation election...  \n",
       "...                                                  ...  \n",
       "21869  news sell, propaganda. e paper straight phone ...  \n",
       "21870                      iwillvote.com registertovote   \n",
       "21871   allinwithchris chrislhayes wow. 63 year. cong...  \n",
       "21872   registertovote votebluetosavedemocracy votebl...  \n",
       "21873  wake america, 69 day left midterm. votebluetos...  \n",
       "\n",
       "[21873 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contestant_map = {'johnfetterman':'fetterman','john':'fetterman','fettermanforsenate':'fetterman','teamfetterman':'fetterman',\n",
    "                    'fettermanisthebetterman':'fetterman','fetterman2022':'fetterman','giselefetterman':'fetterman','levifetterman':'fetterman',\n",
    "                    'votefetterman':'fetterman','fettermanpa':'fetterman','johnfettermancom':'fetterman','fettermanforpa':'fetterman',\n",
    "                    'vote4fetterman':'fetterman','seniors4fetterman':'fetterman','johnfettermanpa':'fetterman','johnfettermanforpa':'fetterman',\n",
    "                    'flushfetterman':'fetterman','johnfettermanforsenate':'fetterman','fettermanthebetterman':'fetterman','fettermanisafraud':'fetterman',\n",
    "                    'votejohnfetterman':'fetterman','fetterman4pa':'fetterman','votefettermanthebetterman':'fetterman','wheresfetterman':'fetterman',\n",
    "                    'droz':'oz','mehmet':'oz','mehmetoz':'oz','noozhere':'oz','mehmetoz':'oz','stopoz':'oz','notoz':'oz','doctorozcom':'oz,',\n",
    "                    'rejectoz':'oz','nooz':'oz','ozforsenate':'oz','ozforpa':'oz','newjerseyoz':'oz','ozsucks':'oz','drozforsenate':'oz',\n",
    "                    'voteoz':'oz','teamoz':'oz','drmehmetoz':'oz','ozisafraud':'oz','sendozbacktonj':'oz','drozfornj':'oz','ozthefraud':'oz',\n",
    "                    'notodroz':'oz','mehoz':'oz','jerseyoz':'oz','doctoroz':'oz','ozfornj':'oz','drozisafraud':'oz','oz4sen':'oz'}\n",
    "\n",
    "import re\n",
    "\n",
    "def replace(match):\n",
    "    return contestant_map[match.group(0)]\n",
    "\n",
    "def replace_contestant_names_with_similar_ones(text):\n",
    "    text = re.sub('|'.join(r'\\b%s\\b' % re.escape(s) for s in contestant_map), replace, text)\n",
    "    return text\n",
    "\n",
    "df_temp_tweets['clean'] = df_temp_tweets['clean'].map(replace_contestant_names_with_similar_ones)\n",
    "df_temp_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B\n",
    "#### Getting the top for issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>registertovote</th>\n",
       "      <td>8947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasen</th>\n",
       "      <td>5222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vote</th>\n",
       "      <td>3486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022midterms</th>\n",
       "      <td>3310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fetterman</th>\n",
       "      <td>2695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midtermelections</th>\n",
       "      <td>1905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>1711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz</th>\n",
       "      <td>1685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>election</th>\n",
       "      <td>1659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count\n",
       "word                   \n",
       "registertovote     8947\n",
       "pasen              5222\n",
       "vote               3486\n",
       "2022midterms       3310\n",
       "fetterman          2695\n",
       "midtermelections   1905\n",
       "and                1762\n",
       "amp                1711\n",
       "oz                 1685\n",
       "election           1659"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate unique word frequency, keeping words unique for a single comment ie, is Fetterman appear twice in a comment for that comment frequency of Fetterman will be taken as 1\n",
    "def unique_freq(Series):\n",
    "    \n",
    "    y = Series.map(lambda x: list(set(x.lstrip('\"').rstrip('\"').split())))\n",
    "\n",
    "    wordlists_list = list(y)\n",
    "    freq_dict = {}\n",
    "    for i in range (len(wordlists_list)):\n",
    "        for j in range(len(wordlists_list[i])):\n",
    "            if wordlists_list[i][j] in freq_dict:\n",
    "                freq_dict[wordlists_list[i][j]] = freq_dict[wordlists_list[i][j]] + 1\n",
    "            else:\n",
    "                freq_dict[wordlists_list[i][j]] = 1\n",
    "\n",
    "    uniq_freq = pd.DataFrame(list(freq_dict.items()),columns = ['word','count']).set_index('word')\n",
    "    return uniq_freq\n",
    "\n",
    "# using above function to get unique word frequencies on Text_clean which has no stopwords and models replaced with\n",
    "#brands. This result can be used to determine our top 4 issues.\n",
    "\n",
    "df_unique_freq = unique_freq(df_temp_tweets['clean']).sort_values(by = 'count', ascending = False)\n",
    "df_unique_freq.to_csv('Unique_Word_freq.csv')\n",
    "df_unique_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 4 issues we found are:\n",
    "* inflation\n",
    "* abortion\n",
    "* crime\n",
    "* job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D\n",
    "#### Find Lift and Sentiment for contestant vs issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the lift values: Contestants vs Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inflation</th>\n",
       "      <th>abortion</th>\n",
       "      <th>crime</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fetterman</th>\n",
       "      <td>0.222899</td>\n",
       "      <td>1.268147</td>\n",
       "      <td>3.399431</td>\n",
       "      <td>2.254484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz</th>\n",
       "      <td>0.209709</td>\n",
       "      <td>1.582060</td>\n",
       "      <td>1.427231</td>\n",
       "      <td>1.854430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           inflation  abortion     crime       job\n",
       "fetterman   0.222899  1.268147  3.399431  2.254484\n",
       "oz          0.209709  1.582060  1.427231  1.854430"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function takes a string, a list, dataframe from which a column with comments is specified and freq_series which has \n",
    "# the word unique frequencies and returns lift values of the string with all the values present in the list\n",
    "\n",
    "def lift(string, string_list,dataf,column,freq_series):\n",
    "    nrows = float(len(dataf))\n",
    "    string_n = float(freq_series.loc[string])\n",
    "    string_mask = dataf[column].str.contains(string)\n",
    "    lift_list = []\n",
    "    for items in string_list:\n",
    "        if string == items:\n",
    "            lift_list.append('NA')\n",
    "        else:\n",
    "            string_list_n = float(freq_series.loc[items])\n",
    "            mel_count = float(dataf[string_mask][column].str.contains(items).sum())\n",
    "            lift = (nrows * mel_count)/(string_n * string_list_n)\n",
    "            lift_list.append(lift)\n",
    "    \n",
    "    return lift_list\n",
    "\n",
    "issue_list = ['inflation','abortion','crime','job']\n",
    "contestants = ['fetterman','oz']\n",
    "\n",
    "from time import sleep\n",
    "lift_dict = {}\n",
    "for i in range(len(contestants)):\n",
    "    lift_dict[contestants[i]] = lift(contestants[i], issue_list,df_temp_tweets,'clean',df_unique_freq)\n",
    "\n",
    "\n",
    "issues_lift = pd.DataFrame.from_dict(lift_dict)\n",
    "issues_lift.index = issue_list\n",
    "issues_lift = issues_lift.T\n",
    "issues_lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple of things to remark basis the above Lift association. We will just highlight the observations here and would wish to map it with actual inferences and insights at the end of the file so the story stiches out well - \n",
    "\n",
    "* There is a marked high Lift value (~3.4)between Fetterman and Crime\n",
    "* For Dr. Oz, there is a relatively high Lift value with Abortion and Jobs (may be generalized to economy) too "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment ###\n",
    "Found the sentiment for every tweets, then filter out tweets with the contestant and issue related keywords and get the average of the sentiments of that tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['inflation,fetterman', 'inflation,oz', 'abortion,fetterman',\n",
       "       'abortion,oz', 'crime,fetterman', 'crime,oz', 'job,fetterman',\n",
       "       'job,oz'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dictionary of the keywords required to form the output dataframe for lift comparison\n",
    "issues_dict_temp = issues_lift.to_dict()\n",
    "issues_dict = {}\n",
    "for key in issues_dict_temp:\n",
    "    for k in issues_dict_temp[key]:\n",
    "        issues_dict[key + \",\" +k]=issues_dict_temp[key][k]\n",
    "\n",
    "df_issues = pd.DataFrame.from_dict(issues_dict, orient='index', columns=['Lift'])\n",
    "df_issues.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sanjoshaju/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>clean</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...</td>\n",
       "      <td>adamcarolla timcast pnjaban 2022midterms 2a s...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...</td>\n",
       "      <td>adamcarolla timcast pnjaban anncoulter senate...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private Prison Sues State for Not Having Enoug...</td>\n",
       "      <td>private prison sue state prisoner wondergressi...</td>\n",
       "      <td>-0.7783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can we just admit that the American food lifes...</td>\n",
       "      <td>admit american food lifestyle promotes diabete...</td>\n",
       "      <td>-0.5994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kudos To Karine bit.ly/3cItPUL #inflation #ele...</td>\n",
       "      <td>kudos karine bit.ly 3citpul inflation election...</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21869</th>\n",
       "      <td>News is what we sell, not propaganda ... Get o...</td>\n",
       "      <td>news sell, propaganda. e paper straight phone ...</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21870</th>\n",
       "      <td>IWillVote.com #RegisterToVote</td>\n",
       "      <td>iwillvote.com registertovote</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21871</th>\n",
       "      <td>@allinwithchris @chrislhayes Wow. It only took...</td>\n",
       "      <td>allinwithchris chrislhayes wow. 63 year. cong...</td>\n",
       "      <td>0.7783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21872</th>\n",
       "      <td>#RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...</td>\n",
       "      <td>registertovote votebluetosavedemocracy votebl...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21873</th>\n",
       "      <td>Wake up #America, there are only 69 days left ...</td>\n",
       "      <td>wake america, 69 day left midterm. votebluetos...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21873 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         renderedContent  \\\n",
       "0      @adamcarolla @Timcast @pnjaban \\n#2022MIDTERMS...   \n",
       "1      @adamcarolla @Timcast @pnjaban\\n@AnnCoulter\\n ...   \n",
       "2      Private Prison Sues State for Not Having Enoug...   \n",
       "3      Can we just admit that the American food lifes...   \n",
       "4      Kudos To Karine bit.ly/3cItPUL #inflation #ele...   \n",
       "...                                                  ...   \n",
       "21869  News is what we sell, not propaganda ... Get o...   \n",
       "21870                      IWillVote.com #RegisterToVote   \n",
       "21871  @allinwithchris @chrislhayes Wow. It only took...   \n",
       "21872  #RegisterToVote \\n#VoteBlueToSaveDemocracy \\n#...   \n",
       "21873  Wake up #America, there are only 69 days left ...   \n",
       "\n",
       "                                                   clean  Polarity  \n",
       "0       adamcarolla timcast pnjaban 2022midterms 2a s...    0.0000  \n",
       "1       adamcarolla timcast pnjaban anncoulter senate...    0.0000  \n",
       "2      private prison sue state prisoner wondergressi...   -0.7783  \n",
       "3      admit american food lifestyle promotes diabete...   -0.5994  \n",
       "4      kudos karine bit.ly 3citpul inflation election...    0.5106  \n",
       "...                                                  ...       ...  \n",
       "21869  news sell, propaganda. e paper straight phone ...    0.4939  \n",
       "21870                      iwillvote.com registertovote     0.0000  \n",
       "21871   allinwithchris chrislhayes wow. 63 year. cong...    0.7783  \n",
       "21872   registertovote votebluetosavedemocracy votebl...    0.0000  \n",
       "21873  wake america, 69 day left midterm. votebluetos...    0.0000  \n",
       "\n",
       "[21873 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the sentiment for all the tweets\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "new_words = {\n",
    "    'oz': 0,\n",
    "    'fetterman': 0,\n",
    "    'smalltownpa':0,\n",
    "    'bigtownpa':0\n",
    "}\n",
    "\n",
    "sent_analyser = SentimentIntensityAnalyzer()\n",
    "sent_analyser.lexicon.update(new_words)\n",
    "def sentiment(text):\n",
    "    return (sent_analyser.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "df_temp_tweets['Polarity'] = df_temp_tweets['clean'].map(sentiment)\n",
    "df_temp_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing the tweets to sentiment based on the polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def senti(data):\n",
    "    if data >= 0.05:\n",
    "        val = \"Positive\"\n",
    "    elif data <= -0.05:\n",
    "        val = \"Negative\"\n",
    "    else:\n",
    "        val = \"Neutral\"\n",
    "    return val\n",
    "\n",
    "df_temp_tweets['Sentiment'] = df_temp_tweets['Polarity'].map(senti)\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.countplot(x=\"Sentiment\", data=df_temp_tweets, \n",
    "                  palette=dict(Neutral=\"blue\", Positive=\"Green\", Negative=\"Red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Lift and sentiment output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lift</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inflation,fetterman</th>\n",
       "      <td>0.222899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation,oz</th>\n",
       "      <td>0.209709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abortion,fetterman</th>\n",
       "      <td>1.268147</td>\n",
       "      <td>-0.100063</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abortion,oz</th>\n",
       "      <td>1.582060</td>\n",
       "      <td>-0.400726</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime,fetterman</th>\n",
       "      <td>3.399431</td>\n",
       "      <td>-0.684062</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime,oz</th>\n",
       "      <td>1.427231</td>\n",
       "      <td>-0.506547</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job,fetterman</th>\n",
       "      <td>2.254484</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job,oz</th>\n",
       "      <td>1.854430</td>\n",
       "      <td>-0.090436</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lift  Polarity Sentiment\n",
       "inflation,fetterman  0.222899       NaN       NaN\n",
       "inflation,oz         0.209709       NaN       NaN\n",
       "abortion,fetterman   1.268147 -0.100063  Negative\n",
       "abortion,oz          1.582060 -0.400726  Negative\n",
       "crime,fetterman      3.399431 -0.684062  Negative\n",
       "crime,oz             1.427231 -0.506547  Negative\n",
       "job,fetterman        2.254484  0.007070   Neutral\n",
       "job,oz               1.854430 -0.090436  Negative"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "df_temp_tweets['clean_tokenized'] = df_temp_tweets['clean'].map(tokenize_text)\n",
    "df_temp_tweets\n",
    "\n",
    "def filter_fn(row, t):\n",
    "    t = t.split(\",\")\n",
    "    if all(x in row['clean_tokenized'] for x in t):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def find_sentiment(text):\n",
    "    m = df_temp_tweets.apply(lambda x: filter_fn(x, text), axis=1)\n",
    "    df1 = df_temp_tweets[m]\n",
    "\n",
    "    return df1[\"Polarity\"].mean()\n",
    "\n",
    "df_issues[\"Polarity\"] = df_issues.index.map(find_sentiment)\n",
    "df_issues['Sentiment'] = df_issues['Polarity'].map(senti)\n",
    "\n",
    "df_issues.loc[df_issues['Lift'] < 1, 'Polarity'] = np.nan\n",
    "df_issues.loc[df_issues['Lift'] < 1, 'Sentiment'] = np.nan\n",
    "\n",
    "df_issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple of things to remark basis the above Lift and Sentiment association. Like Lift, we will just highlight the observations here and would wish to map it with actual inferences and insights at the end of the file so the story stiches out well - \n",
    "\n",
    "* There is a marked high Lift value (~3.4) backed up by negative sentiment between Fetterman and Crime\n",
    "* For Dr. Oz, there is a relatively high Lift value with Abortion and backed up by negative sentiment\n",
    "* The only 'neutral' sentiment depicted is by the association between Fetterman and Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS plot: Contestants vs Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mds_attr_list = ['inflation','abortion','crime','job','fetterman','oz']\n",
    "\n",
    "from time import sleep\n",
    "lift_dict = {}\n",
    "for i in range(len(mds_attr_list)):\n",
    "    lift_dict[mds_attr_list[i]] = lift(mds_attr_list[i], mds_attr_list,df_temp_tweets,'clean',df_unique_freq)\n",
    "\n",
    "\n",
    "mds_attr_lift = pd.DataFrame.from_dict(lift_dict)\n",
    "mds_attr_lift.index = mds_attr_list\n",
    "# mds_attr_lift\n",
    "\n",
    "# Take reciprocal of the values\n",
    "def div(x):\n",
    "    if x == 'NA':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/x\n",
    "df_scaled = mds_attr_lift.applymap(div)\n",
    "\n",
    "a = df_scaled.to_numpy()\n",
    "\n",
    "# Applying MDS\n",
    "from sklearn.manifold import MDS\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "X_scaled = (a - np.min(a))/np.ptp(a)\n",
    "\n",
    "mds = MDS(2, random_state=0, dissimilarity='precomputed')\n",
    "X_mds = mds.fit_transform(X_scaled)\n",
    "\n",
    "#  Plotting MDS\n",
    "x= X_mds[:,0]\n",
    "y= X_mds[:,1]\n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i],c='blue')\n",
    "    plt.annotate(\n",
    "        mds_attr_lift.index[i],\n",
    "        xy = (x[i], y[i]), xytext = (25, 3),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe in the MDS plots too and notice the marked closeness between specifically -\n",
    "\n",
    "* Dr. Oz and Abortion\n",
    "* Fetterman and Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the lift values: Contestants vs Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smalltownpa</th>\n",
       "      <th>bigtownpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fetterman</th>\n",
       "      <td>1.785551</td>\n",
       "      <td>2.618110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz</th>\n",
       "      <td>2.076961</td>\n",
       "      <td>7.118618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           smalltownpa  bigtownpa\n",
       "fetterman     1.785551   2.618110\n",
       "oz            2.076961   7.118618"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function takes a string, a list, dataframe from which a column with comments is specified and freq_series which has \n",
    "# the word unique frequencies and returns lift values of the string with all the values present in the list\n",
    "\n",
    "def lift(string, string_list,dataf,column,freq_series):\n",
    "    nrows = float(len(dataf))\n",
    "    string_n = float(freq_series.loc[string])\n",
    "    string_mask = dataf[column].str.contains(string)\n",
    "    lift_list = []\n",
    "    for items in string_list:\n",
    "        if string == items:\n",
    "            lift_list.append('NA')\n",
    "        else:\n",
    "            string_list_n = float(freq_series.loc[items])\n",
    "            mel_count = float(dataf[string_mask][column].str.contains(items).sum())\n",
    "            lift = (nrows * mel_count)/(string_n * string_list_n)\n",
    "            lift_list.append(lift)\n",
    "    \n",
    "    return lift_list\n",
    "\n",
    "location_list = ['smalltownpa','bigtownpa']\n",
    "contestants = ['fetterman','oz']\n",
    "\n",
    "from time import sleep\n",
    "lift_dict = {}\n",
    "for i in range(len(contestants)):\n",
    "    lift_dict[contestants[i]] = lift(contestants[i], location_list,df_temp_tweets,'clean',df_unique_freq)\n",
    "\n",
    "\n",
    "location_lift = pd.DataFrame.from_dict(lift_dict)\n",
    "location_lift.index = location_list\n",
    "location_lift = location_lift.T\n",
    "location_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lift</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smalltownpa,fetterman</th>\n",
       "      <td>1.785551</td>\n",
       "      <td>0.203718</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smalltownpa,oz</th>\n",
       "      <td>2.076961</td>\n",
       "      <td>0.228750</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigtownpa,fetterman</th>\n",
       "      <td>2.618110</td>\n",
       "      <td>-0.169840</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigtownpa,oz</th>\n",
       "      <td>7.118618</td>\n",
       "      <td>0.062594</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lift  Polarity Sentiment\n",
       "smalltownpa,fetterman  1.785551  0.203718  Positive\n",
       "smalltownpa,oz         2.076961  0.228750  Positive\n",
       "bigtownpa,fetterman    2.618110 -0.169840  Negative\n",
       "bigtownpa,oz           7.118618  0.062594  Positive"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_dict_temp = location_lift.to_dict()\n",
    "location_dict = {}\n",
    "for key in location_dict_temp:\n",
    "    for k in location_dict_temp[key]:\n",
    "        location_dict[key + \",\" +k]=location_dict_temp[key][k]\n",
    "\n",
    "df_location = pd.DataFrame.from_dict(location_dict, orient='index', columns=['Lift'])\n",
    "df_location.index\n",
    "\n",
    "def filter_fn(row, t):\n",
    "    t = t.split(\",\")\n",
    "    if all(x in row['clean_tokenized'] for x in t):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def find_sentiment(text):\n",
    "    m = df_temp_tweets.apply(lambda x: filter_fn(x, text), axis=1)\n",
    "    df1 = df_temp_tweets[m]\n",
    "\n",
    "    return df1[\"Polarity\"].mean()\n",
    "\n",
    "df_location[\"Polarity\"] = df_location.index.map(find_sentiment)\n",
    "df_location['Sentiment'] = df_location['Polarity'].map(senti)\n",
    "df_location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task G:\n",
    "#### Insights basis above Lift Association, Sentiment Analysis and MDS plots -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 4 issues highlighted for this election in PA are -\n",
    "* inflation\n",
    "* abortion\n",
    "* crime\n",
    "* job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetterman should focus on â€œCrimeâ€ as an agenda (we could see that in his Lift, MDS associations), which could orient around Gun control and Violence. Primarily there is negative publicity mounted on by â€œattack adsâ€ and being hammered negatively by Dr. Ozâ€™s team. Public opinion has also linked Fetterman to an incident in 2013 where-in he pulled a shotgun on a black jogger, which could be have severe impacts on the outcome of this election. Additionally, concerns with public safety record and the incidents of violent crime went up under the leadership of Fetterman which further rages up this topic. \n",
    "A positive publicity campaign on any issue pertaining to crime or control of gun related incidents will help Fetterman and propel him in a positive light towards a very crucial aspect of the election i.e. Crime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oz is portrayed as a â€œpeopleâ€™s manâ€œ and so does his activities portray him in the same light. He is a regular contributor to â€œOprah Winfreyâ€ show and opinion portrays him to be successful in connecting to financially unfortunate people. Additionally, he has fostered activities towards Economy (or job creation to be specific) by spending time with local black entrepreneurs. This could prove pivotal in events leading up to boosting economy, jobs, overall market sentiment, taming inflation and other such concerns.\n",
    "The concern area which is currently under a negative sentiment is his statement pertaining to Abortion issues (we could see that in his Lift, MDS associations) where-in he had stated in a press conference held in Philadelphia that he would not support criminal penalties for people who sought or doctors who performed abortions. Describing himself as â€œstrongly pro-life,â€ he added that he supports exceptions for rape, incest, or if the motherâ€™s life is at risk. Oz has faced attack over his statements from the Fettermanâ€™s side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights on Overall outcome of election â€“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that Fetterman is favoured to win the election, opinions indicate a narrowing of margins between the two. This can be affirmed by an excerpt from an article too - \n",
    "According to the Deluxe version of the FiveThirtyEight forecast, Fettermanâ€™s chances of being the next junior senator from Pennsylvania have diminished, from 83 in 100 as recently as Sept. 24 to 73 in 100 today.\n",
    "Hence, it will be interesting to drive this outcome basis the insights suggested and hope to see the recommended action."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
